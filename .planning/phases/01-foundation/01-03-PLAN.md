---
phase: 01-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - src/qm_nmr_calc/startup.py
  - scripts/run_consumer.py
  - scripts/test_workflow.py
autonomous: true

must_haves:
  truths:
    - "Service validates environment at startup and fails fast if NWChem unavailable"
    - "Interrupted jobs are recovered on startup (marked as failed)"
    - "A job can be queued, picked up by consumer, and completes with optimized geometry"
    - "Failed jobs have clear error status and message"
  artifacts:
    - path: "src/qm_nmr_calc/startup.py"
      provides: "Environment validation and recovery logic"
      exports: ["validate_environment", "recover_interrupted_jobs"]
    - path: "scripts/run_consumer.py"
      provides: "Consumer startup script with validation"
      min_lines: 15
    - path: "scripts/test_workflow.py"
      provides: "Integration test script"
      min_lines: 40
  key_links:
    - from: "scripts/run_consumer.py"
      to: "src/qm_nmr_calc/startup.py"
      via: "calls validate_environment on startup"
      pattern: "validate_environment"
    - from: "scripts/test_workflow.py"
      to: "src/qm_nmr_calc/storage.py"
      via: "creates job via create_job_directory"
      pattern: "create_job_directory"
    - from: "scripts/test_workflow.py"
      to: "src/qm_nmr_calc/tasks.py"
      via: "enqueues run_optimization_task"
      pattern: "run_optimization_task"
---

<objective>
Implement startup validation, interrupted job recovery, and verify the complete workflow with an integration test.

Purpose: Ensure the system is production-ready with proper environment checks and graceful handling of process restarts. Validate that the entire Phase 1 goal is achieved.

Output: Working end-to-end calculation workflow - queue a molecule, run geometry optimization, get results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement startup validation and recovery</name>
  <files>src/qm_nmr_calc/startup.py</files>
  <action>
Create `src/qm_nmr_calc/startup.py` with environment validation and interrupted job recovery:

```python
"""Startup validation and recovery logic."""
import sys
from datetime import datetime
from pathlib import Path

import isicle

from .isicle_wrapper import validate_nwchem
from .storage import DATA_DIR, list_jobs_by_status, update_job_status


def validate_environment() -> None:
    """
    Validate environment at startup. Exits if validation fails.

    Checks:
    1. NWChem is available and callable
    2. ISiCLE can initialize (RDKit works)
    3. Data directory is writable
    """
    print("Validating environment...")

    # 1. Check NWChem
    validate_nwchem()
    print("  [OK] NWChem found")

    # 2. Check ISiCLE/RDKit
    try:
        geom = isicle.load("C")  # Simplest molecule
        geom.initial_optimize(embed=True)
        print("  [OK] ISiCLE/RDKit working")
    except Exception as e:
        sys.exit(f"FATAL: ISiCLE/RDKit initialization failed: {e}")

    # 3. Check data directory writable
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    test_file = DATA_DIR / '.write_test'
    try:
        test_file.write_text('test')
        test_file.unlink()
        print(f"  [OK] Data directory writable: {DATA_DIR}")
    except Exception as e:
        sys.exit(f"FATAL: Cannot write to {DATA_DIR}: {e}")

    print("Environment validation passed\n")


def recover_interrupted_jobs() -> int:
    """
    Mark any 'running' jobs as failed on startup.

    This handles the case where the consumer was killed (SIGKILL)
    and SIGNAL_INTERRUPTED didn't fire.

    Returns:
        Number of jobs recovered (marked as failed)
    """
    running_jobs = list_jobs_by_status('running')

    for job_id in running_jobs:
        try:
            update_job_status(
                job_id,
                status='failed',
                completed_at=datetime.utcnow(),
                error_message='Job interrupted - process restart (recovered on startup)'
            )
            print(f"Recovered interrupted job: {job_id}")
        except Exception as e:
            print(f"Warning: Failed to recover job {job_id}: {e}")

    if running_jobs:
        print(f"Recovered {len(running_jobs)} interrupted job(s)\n")

    return len(running_jobs)


def startup() -> None:
    """
    Full startup sequence: validate environment and recover interrupted jobs.
    Call this before starting the consumer.
    """
    validate_environment()
    recover_interrupted_jobs()
```

Key points:
- validate_environment() exits on failure (fail-fast)
- recover_interrupted_jobs() scans for 'running' jobs on startup
- startup() combines both for consumer startup
- Clear messages for each check
  </action>
  <verify>
```bash
# Test validation passes
uv run python -c "from qm_nmr_calc.startup import validate_environment; validate_environment()"

# Test recovery (should find 0 jobs on fresh system)
uv run python -c "from qm_nmr_calc.startup import recover_interrupted_jobs; n = recover_interrupted_jobs(); print(f'Recovered: {n}')"
```
Validation should pass, recovery should show 0 jobs.
  </verify>
  <done>
- validate_environment() checks NWChem, ISiCLE, and data directory
- recover_interrupted_jobs() marks 'running' jobs as failed
- startup() provides combined startup sequence
- Clear error messages on validation failure
  </done>
</task>

<task type="auto">
  <name>Task 2: Create consumer startup script</name>
  <files>scripts/run_consumer.py</files>
  <action>
Create `scripts/run_consumer.py` as the consumer entry point:

```python
#!/usr/bin/env python
"""
Run the Huey consumer with startup validation.

Usage:
    uv run python scripts/run_consumer.py

Or directly:
    python scripts/run_consumer.py

The consumer will:
1. Validate environment (NWChem, ISiCLE, directories)
2. Recover any interrupted jobs from previous runs
3. Start processing queued tasks
"""
import subprocess
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from qm_nmr_calc.startup import startup
from qm_nmr_calc.isicle_wrapper import get_versions


def main():
    print("=" * 50)
    print("QM NMR Calculation Service - Consumer")
    print("=" * 50 + "\n")

    # Run startup checks
    startup()

    # Print version info
    versions = get_versions()
    print(f"ISiCLE version: {versions.isicle}")
    print(f"NWChem version: {versions.nwchem}")
    print()

    # Start Huey consumer
    # -w 1: Single worker (QM jobs are CPU-bound)
    # -k process: Process-based workers (not greenlet)
    print("Starting Huey consumer...")
    print("Press Ctrl+C to stop\n")

    subprocess.run([
        sys.executable, '-m', 'huey.bin.huey_consumer',
        'qm_nmr_calc.queue.huey',
        '-w', '1',
        '-k', 'process',
    ])


if __name__ == '__main__':
    main()
```

Make it executable: `chmod +x scripts/run_consumer.py`

Also create scripts directory: `mkdir -p scripts`
  </action>
  <verify>
```bash
# Create scripts directory if needed
mkdir -p /home/christoph_steinbeck_gmail_com/develop/qm-nmr-calc/scripts

# Test the script can be parsed (don't run consumer, just import)
uv run python -c "
import sys
sys.path.insert(0, 'src')
from qm_nmr_calc.startup import startup
from qm_nmr_calc.isicle_wrapper import get_versions
print('Script dependencies available')
"
```
Should confirm dependencies are available.
  </verify>
  <done>
- Consumer script runs startup validation before starting
- Uses single worker (-w 1) for CPU-bound QM calculations
- Prints version info for reproducibility
- Clear startup messages
  </done>
</task>

<task type="auto">
  <name>Task 3: Create integration test script</name>
  <files>scripts/test_workflow.py</files>
  <action>
Create `scripts/test_workflow.py` to test the complete workflow:

```python
#!/usr/bin/env python
"""
Integration test for the complete calculation workflow.

This script:
1. Creates a job for a simple molecule (ethanol)
2. Enqueues the task
3. Waits for completion (with timeout)
4. Verifies the result

Usage:
    # First, start the consumer in another terminal:
    uv run python scripts/run_consumer.py

    # Then run this test:
    uv run python scripts/test_workflow.py

For a quick test without full DFT (force-field only):
    uv run python scripts/test_workflow.py --quick
"""
import argparse
import sys
import time
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from qm_nmr_calc import (
    create_job_directory,
    load_job_status,
    get_versions,
    run_optimization_task,
)
from qm_nmr_calc.storage import get_job_dir


def test_quick_workflow():
    """Quick test: just verify job creation and ISiCLE loading."""
    print("\n=== Quick Workflow Test ===\n")

    versions = get_versions()
    print(f"ISiCLE: {versions.isicle}, NWChem: {versions.nwchem}")

    # Test 1: Create job
    smiles = "CCO"  # Ethanol
    job = create_job_directory(smiles, versions.isicle, versions.nwchem)
    print(f"Created job: {job.job_id}")
    print(f"Status: {job.status}")

    # Test 2: Verify directory structure
    job_dir = get_job_dir(job.job_id)
    assert (job_dir / 'status.json').exists(), "status.json missing"
    assert (job_dir / 'output').is_dir(), "output/ missing"
    assert (job_dir / 'logs').is_dir(), "logs/ missing"
    print("Directory structure: OK")

    # Test 3: Load job back
    loaded = load_job_status(job.job_id)
    assert loaded is not None, "Failed to load job"
    assert loaded.input.smiles == smiles, "SMILES mismatch"
    print("Job persistence: OK")

    print("\n[PASS] Quick workflow test passed\n")
    return True


def test_full_workflow(timeout_seconds: int = 600):
    """
    Full test: submit job, run calculation, verify result.

    Requires consumer to be running in another process.
    """
    print("\n=== Full Workflow Test ===\n")
    print(f"Timeout: {timeout_seconds} seconds")
    print("NOTE: Consumer must be running in another terminal\n")

    versions = get_versions()

    # Test molecule: ethanol (small but real)
    smiles = "CCO"

    # Step 1: Create job
    job = create_job_directory(smiles, versions.isicle, versions.nwchem)
    print(f"Created job: {job.job_id}")
    print(f"SMILES: {smiles}")

    # Step 2: Enqueue task
    result = run_optimization_task(job.job_id)
    print(f"Task enqueued")

    # Step 3: Wait for completion
    print(f"\nWaiting for completion...")
    start_time = time.time()

    while True:
        status = load_job_status(job.job_id)

        if status.status == 'complete':
            elapsed = time.time() - start_time
            print(f"\n[COMPLETE] Job finished in {elapsed:.1f}s")
            break

        if status.status == 'failed':
            print(f"\n[FAILED] Job failed: {status.error_message}")
            if status.error_traceback:
                print(f"\nTraceback:\n{status.error_traceback}")
            return False

        elapsed = time.time() - start_time
        if elapsed > timeout_seconds:
            print(f"\n[TIMEOUT] Job did not complete within {timeout_seconds}s")
            print(f"Current status: {status.status}")
            return False

        # Print progress every 30 seconds
        if int(elapsed) % 30 == 0 and int(elapsed) > 0:
            print(f"  ... still {status.status} ({int(elapsed)}s)")

        time.sleep(5)

    # Step 4: Verify result
    job_dir = get_job_dir(job.job_id)
    output_file = job_dir / 'output' / 'optimized.xyz'

    if not output_file.exists():
        print(f"[FAIL] Output file missing: {output_file}")
        return False

    # Read and display output
    content = output_file.read_text()
    lines = content.strip().split('\n')
    n_atoms = int(lines[0]) if lines else 0
    print(f"\nOutput file: {output_file}")
    print(f"Atoms: {n_atoms}")
    print(f"First 5 lines:\n{chr(10).join(lines[:5])}")

    # Verify expected atoms for ethanol (C2H6O = 9 atoms)
    if n_atoms != 9:
        print(f"\n[WARN] Expected 9 atoms for ethanol, got {n_atoms}")

    print("\n[PASS] Full workflow test passed\n")
    return True


def main():
    parser = argparse.ArgumentParser(description='Test calculation workflow')
    parser.add_argument('--quick', action='store_true',
                        help='Quick test (no DFT calculation)')
    parser.add_argument('--timeout', type=int, default=600,
                        help='Timeout in seconds for full test (default: 600)')
    args = parser.parse_args()

    if args.quick:
        success = test_quick_workflow()
    else:
        success = test_full_workflow(args.timeout)

    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
```

Make executable: `chmod +x scripts/test_workflow.py`
  </action>
  <verify>
```bash
# Run quick test (no consumer needed)
cd /home/christoph_steinbeck_gmail_com/develop/qm-nmr-calc && uv run python scripts/test_workflow.py --quick
```
Should pass and show job creation and directory structure verification.

Note: Full test requires consumer running - document this in output.
  </verify>
  <done>
- Quick test validates job creation and persistence without calculation
- Full test demonstrates complete workflow (requires consumer)
- Clear progress output during long calculations
- Proper error reporting for failed jobs
- Timeout handling for hung calculations
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Quick verification (no consumer):**
```bash
uv run python scripts/test_workflow.py --quick
```
Should pass immediately, verifying job creation and structure.

2. **Full verification (requires consumer):**

Terminal 1:
```bash
uv run python scripts/run_consumer.py
```

Terminal 2:
```bash
uv run python scripts/test_workflow.py --timeout 600
```

Full test may take 5-10 minutes for ethanol geometry optimization.

3. **Recovery test:**
- Start consumer
- Create a job and enqueue it
- Kill consumer with SIGKILL while job is running
- Restart consumer
- Verify the interrupted job is marked as failed
</verification>

<success_criteria>
Phase 1 Success Criteria (from ROADMAP.md):
1. [x] Job can be queued and executed in background (Huey consumer picks up and runs it)
2. [x] ISiCLE/NWChem runs geometry optimization on a test molecule
3. [x] Failed calculations produce clear error status and message (not silent failure)
4. [x] Job state persists across process restarts (Huey/SQLite)

All criteria verified by:
- Quick test: Job creation and persistence
- Full test: End-to-end geometry optimization
- Recovery logic: Interrupted jobs marked as failed
- Error handling: Exceptions captured with traceback in status.json
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`

This completes Phase 1: Foundation. The system can:
- Queue jobs with full metadata
- Execute geometry optimization via ISiCLE/NWChem
- Track status through Huey signals
- Handle failures gracefully
- Recover from process crashes

Ready for Phase 2: Input and API (REST endpoints for job submission and status).
</output>
