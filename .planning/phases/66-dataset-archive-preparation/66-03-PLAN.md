---
phase: 66-dataset-archive-preparation
plan: 03
type: execute
wave: 2
depends_on: ["66-01", "66-02"]
files_modified:
  - publications/dataset/CHECKSUMS.sha256
  - publications/dataset/MANIFEST.csv
  - publications/dataset/metadata.json
  - scripts/generate_manifest.py
autonomous: true

must_haves:
  truths:
    - "CHECKSUMS.sha256 contains SHA-256 hashes for every file in the dataset archive"
    - "MANIFEST.csv lists every file with filepath, size_bytes, and sha256 columns"
    - "metadata.json has been updated with complete molecule SMILES/InChI/InChIKey from Plan 01 output"
    - "Running sha256sum -c CHECKSUMS.sha256 from the dataset root verifies all files"
  artifacts:
    - path: "publications/dataset/CHECKSUMS.sha256"
      provides: "SHA-256 checksums for all files"
      min_lines: 600
    - path: "publications/dataset/MANIFEST.csv"
      provides: "File manifest with sizes and checksums"
      min_lines: 600
    - path: "scripts/generate_manifest.py"
      provides: "Reproducible manifest generation script"
      min_lines: 50
  key_links:
    - from: "scripts/generate_manifest.py"
      to: "publications/dataset/"
      via: "recursive file traversal with SHA-256"
      pattern: "hashlib.sha256"
    - from: "publications/dataset/MANIFEST.csv"
      to: "publications/dataset/CHECKSUMS.sha256"
      via: "same checksums in different format"
      pattern: "sha256"
---

<objective>
Generate file integrity checksums and manifest for the complete dataset archive, and finalize metadata.json with molecule identifiers from Plan 01.

Purpose: Provide verifiable file integrity (DATA-08) and a complete file listing (DATA-09) so that archive consumers can validate download integrity and enumerate all dataset contents. Also merge molecule chemical identifiers into DataCite metadata.

Output: CHECKSUMS.sha256, MANIFEST.csv, updated metadata.json, and the generation script.
</objective>

<execution_context>
@/home/chris/.claude/get-shit-done/workflows/execute-plan.md
@/home/chris/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/66-dataset-archive-preparation/66-RESEARCH.md
@.planning/phases/66-dataset-archive-preparation/66-01-SUMMARY.md
@.planning/phases/66-dataset-archive-preparation/66-02-SUMMARY.md

This plan runs AFTER Plan 01 (data) and Plan 02 (docs) are complete. The publications/dataset/ directory should contain all files at this point.

Expected file count: 610 .nw + 610 .out + molecules.csv + molecules.json + scaling_factors.csv + scaling_factors.json + shielding_tensors.json + computation_times.csv + README.md + PROVENANCE.md + LICENSE + metadata.json = 1228+ files

CHECKSUMS.sha256 format: standard sha256sum output (`hash  filepath` per line)
MANIFEST.csv format: filepath, size_bytes, sha256 (CSV with header row)

MANIFEST.csv and CHECKSUMS.sha256 should NOT include themselves (they can't checksum themselves).
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create manifest generation script and run it</name>
  <files>
    scripts/generate_manifest.py
    publications/dataset/CHECKSUMS.sha256
    publications/dataset/MANIFEST.csv
  </files>
  <action>
1. **Create scripts/generate_manifest.py**:
   - Accept `--dataset-dir` argument (default: publications/dataset/)
   - Recursively traverse all files in dataset directory
   - For each file (excluding MANIFEST.csv and CHECKSUMS.sha256 themselves):
     - Compute SHA-256 checksum using hashlib with 8KB chunk reads
     - Record filepath (relative to dataset root), size_bytes, sha256
   - Sort entries by filepath for deterministic output
   - Write MANIFEST.csv with columns: filepath, size_bytes, sha256
   - Write CHECKSUMS.sha256 in standard sha256sum format: `{hash}  {filepath}`
   - Print summary: total files, total size, elapsed time

2. **Run the script**:
   ```
   python scripts/generate_manifest.py --dataset-dir publications/dataset/
   ```

3. **Validate output**:
   - Verify file count matches expected (~1228 files)
   - Spot-check a few checksums with `sha256sum` CLI
   - Verify MANIFEST.csv is valid CSV with correct headers
   - Verify CHECKSUMS.sha256 format is compatible with `sha256sum -c`
  </action>
  <verify>
- `wc -l publications/dataset/MANIFEST.csv` shows 1220+ lines (header + files)
- `wc -l publications/dataset/CHECKSUMS.sha256` shows 1220+ lines
- `head -3 publications/dataset/MANIFEST.csv` shows "filepath,size_bytes,sha256" header
- `head -3 publications/dataset/CHECKSUMS.sha256` shows hash + filepath format
- `cd publications/dataset && sha256sum -c CHECKSUMS.sha256 --quiet 2>&1 | head -5` shows no errors (or all OK)
  </verify>
  <done>
CHECKSUMS.sha256 and MANIFEST.csv generated for all files in dataset archive. Checksums verified via sha256sum.
  </done>
</task>

<task type="auto">
  <name>Task 2: Finalize metadata.json with molecule identifiers</name>
  <files>publications/dataset/metadata.json</files>
  <action>
1. **Read molecule identifiers from Plan 01 output**:
   - Load `publications/dataset/molecules/molecules.json`
   - Extract SMILES, InChI, InChIKey for all 50 molecules

2. **Update metadata.json chemistry extensions**:
   - Read existing `publications/dataset/metadata.json` (from Plan 02)
   - Add/update the chemistry extensions section with complete molecule identifiers:
     ```json
     "chemistryExtensions": {
       "molecules": [
         {
           "compoundId": "compound_01",
           "name": "Nitromethane",
           "smiles": "...",
           "inchi": "...",
           "inchikey": "..."
         },
         ...
       ],
       "solvents": ["chloroform", "dmso", ...],
       "functionals": ["B3LYP", "WP04"],
       "basisSet": "6-311+G(2d,p)"
     }
     ```
   - Verify all 50 molecules have non-empty SMILES, InChI, InChIKey
   - Write updated metadata.json (pretty-printed, indent=2)

3. **Final dataset validation**:
   - Print summary of the complete archive:
     - Total files
     - Total size
     - Calculations count
     - Molecules with complete identifiers
     - Scaling factor sets count
   - Verify no empty/placeholder fields remain in metadata.json (except DOI which is intentionally placeholder)
  </action>
  <verify>
- `python -c "import json; d=json.load(open('publications/dataset/metadata.json')); print(len(d.get('chemistryExtensions',{}).get('molecules',[])))"` shows 50
- `python -c "import json; d=json.load(open('publications/dataset/metadata.json')); m=d['chemistryExtensions']['molecules'][0]; print(all(m.get(k) for k in ['smiles','inchi','inchikey']))"` shows True
- metadata.json is valid JSON
  </verify>
  <done>
metadata.json updated with complete SMILES/InChI/InChIKey for all 50 molecules. Dataset archive is complete with all DATA-01 through DATA-09 requirements satisfied.
  </done>
</task>

</tasks>

<verification>
1. `wc -l publications/dataset/MANIFEST.csv` shows 1220+ lines
2. `wc -l publications/dataset/CHECKSUMS.sha256` shows 1220+ lines
3. `cd publications/dataset && sha256sum -c CHECKSUMS.sha256 --quiet 2>&1 | tail -1` verifies all checksums
4. metadata.json has 50 molecules with SMILES/InChI/InChIKey in chemistryExtensions
5. `find publications/dataset/ -type f | wc -l` shows total file count
6. `du -sh publications/dataset/` shows total archive size (~100-150MB)
</verification>

<success_criteria>
- Every file in publications/dataset/ (except MANIFEST.csv and CHECKSUMS.sha256) has a SHA-256 checksum
- MANIFEST.csv lists all files with filepath, size, and checksum
- CHECKSUMS.sha256 is compatible with sha256sum -c verification
- metadata.json chemistryExtensions has complete identifiers for all 50 molecules
- Dataset archive is complete and ready for repository upload
</success_criteria>

<output>
After completion, create `.planning/phases/66-dataset-archive-preparation/66-03-SUMMARY.md`
</output>
