---
phase: 66-dataset-archive-preparation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/build_dataset_archive.py
  - publications/dataset/calculations/**
  - publications/dataset/molecules/molecules.csv
  - publications/dataset/molecules/molecules.json
  - publications/dataset/scaling_factors/scaling_factors.csv
  - publications/dataset/scaling_factors/scaling_factors.json
  - publications/dataset/shielding_tensors/shielding_tensors.json
  - publications/dataset/timing/computation_times.csv
autonomous: true

must_haves:
  truths:
    - "publications/dataset/calculations/ contains 610 .nw and 610 .out files organized by compound/solvent hierarchy"
    - "molecules.csv contains all 50 DELTA50 compounds with SMILES, InChI, InChIKey, molecular formula"
    - "scaling_factors.csv contains all 24 benchmark scaling factor sets with slope, intercept, R-squared, MAE"
    - "shielding_tensors.json contains extracted shielding data from all 610 calculations"
    - "computation_times.csv contains wall-clock timing extracted from all 610 .out files"
  artifacts:
    - path: "scripts/build_dataset_archive.py"
      provides: "Reproducible archive build script"
      min_lines: 150
    - path: "publications/dataset/calculations/compound_01/chloroform/shielding.nw"
      provides: "Sample organized NWChem input"
    - path: "publications/dataset/molecules/molecules.csv"
      provides: "Molecule metadata with chemical identifiers"
    - path: "publications/dataset/scaling_factors/scaling_factors.csv"
      provides: "All scaling factors in tabular format"
    - path: "publications/dataset/shielding_tensors/shielding_tensors.json"
      provides: "All shielding tensor data from 610 calculations"
    - path: "publications/dataset/timing/computation_times.csv"
      provides: "Computation time logs"
  key_links:
    - from: "scripts/build_dataset_archive.py"
      to: "data/benchmark/results/"
      via: "file traversal and copy"
      pattern: "data/benchmark/results"
    - from: "scripts/build_dataset_archive.py"
      to: "data/benchmark/delta50/molecules/"
      via: "RDKit XYZ parsing for identifiers"
      pattern: "Chem.MolFromXYZFile"
---

<objective>
Build the dataset archive: organize 610 NWChem calculations into publication-ready hierarchy and generate processed data exports.

Purpose: Create the core data content of the DELTA50 dataset archive with all calculations organized by compound/solvent and processed data exports (molecule identifiers, scaling factors, shielding tensors, timing) for FAIR-compliant publication on Radar4Chem.

Output: Python script `scripts/build_dataset_archive.py` that builds the entire archive, plus the populated `publications/dataset/` directory with calculations and processed data.
</objective>

<execution_context>
@/home/chris/.claude/get-shit-done/workflows/execute-plan.md
@/home/chris/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/66-dataset-archive-preparation/66-RESEARCH.md

Source data locations:
- 610 calculations: data/benchmark/results/compound_XX/B3LYP_SOLVENT/scratch/shielding.{nw,out}
  - 12 B3LYP solvents x 50 compounds = 600
  - WP04_CHCl3 and WP04_DMSO for compound_01-10 = 10 extra (20 calcs but only 10 unique compounds)
  - Solvents: CHCl3, DMSO, Methanol, Water, Acetone, Benzene, Pyridine, THF, Toluene, DCM, Acetonitrile, DMF
  - WP04: CHCl3, DMSO (compound_01-10 only)
- 50 molecule XYZ files: data/benchmark/delta50/molecules/compound_XX.xyz
- Experimental shifts: data/benchmark/delta50/experimental_shifts.json (has molecule names)
- Scaling factors (benchmark): data/benchmark/delta50/scaling_factors.json (24 factor sets, no vacuum)
- Scaling factors (package): src/qm_nmr_calc/data/scaling_factors.json (26 factor sets, includes vacuum)
- shifts.json: 600 exist in B3LYP dirs (not WP04)

CRITICAL: Only copy shielding.nw and shielding.out from scratch/ dirs. Do NOT copy .movecs, .gridpts.*, or other scratch files (would bloat from ~100MB to >5GB).

Solvent name mapping for archive directories (use canonical lowercase names):
- CHCl3 -> chloroform
- DMSO -> dmso
- Methanol -> methanol
- Water -> water
- Acetone -> acetone
- Benzene -> benzene
- Pyridine -> pyridine
- THF -> thf
- Toluene -> toluene
- DCM -> dichloromethane
- Acetonitrile -> acetonitrile
- DMF -> dmf
- vacuum -> vacuum (not present on disk but documented)

The requirements say "650 calculations" but only 610 exist on disk. Vacuum calculations (50) were cleaned up after completion. The archive must document this accurately: 610 calculations from 50 molecules x 12 B3LYP solvents + 10 WP04 calculations.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create archive build script</name>
  <files>scripts/build_dataset_archive.py</files>
  <action>
Create a Python script `scripts/build_dataset_archive.py` that performs the full archive build. The script should:

1. **Organize calculations (DATA-01)**:
   - Traverse `data/benchmark/results/compound_XX/B3LYP_SOLVENT/scratch/` directories
   - Copy ONLY `shielding.nw` and `shielding.out` to `publications/dataset/calculations/compound_XX/{canonical_solvent}/`
   - Also handle WP04 directories: copy to `publications/dataset/calculations/compound_XX/wp04_{canonical_solvent}/`
   - Use canonical lowercase solvent names (see mapping in context)
   - Skip compound_51, compound_52, compound_53 (not part of DELTA50)
   - Print progress with counts

2. **Generate molecule metadata (DATA-02 partial)**:
   - Read 50 XYZ files from `data/benchmark/delta50/molecules/`
   - Use RDKit to generate SMILES, InChI, InChIKey, molecular formula for each
   - Merge molecule names from `data/benchmark/delta50/experimental_shifts.json`
   - Write `publications/dataset/molecules/molecules.csv` with columns: compound_id, name, smiles, inchi, inchikey, molecular_formula, num_h_atoms, num_c_atoms
   - Write `publications/dataset/molecules/molecules.json` with same data

3. **Export scaling factors (DATA-02 partial)**:
   - Read `data/benchmark/delta50/scaling_factors.json` (24 factor sets)
   - Also read `src/qm_nmr_calc/data/scaling_factors.json` for vacuum factors (26 factor sets)
   - Write `publications/dataset/scaling_factors/scaling_factors.csv` with columns: functional, basis_set, nucleus, solvent, slope, intercept, r_squared, mae, rmsd, n_points, outliers_removed
   - Write `publications/dataset/scaling_factors/scaling_factors.json` with same data plus confidence intervals

4. **Export shielding tensors (DATA-02 partial)**:
   - Read all 600 shifts.json files from B3LYP calculation directories
   - Aggregate into `publications/dataset/shielding_tensors/shielding_tensors.json` organized by compound/solvent
   - Include raw isotropic shielding values per atom

5. **Extract computation times (DATA-07)**:
   - Parse all 610 .out files for wall-clock timing using regex: `Total times\s+cpu:\s+[\d.]+s\s+wall:\s+([\d.]+)s`
   - Write `publications/dataset/timing/computation_times.csv` with columns: compound_id, functional, solvent, wall_time_seconds, cpu_time_seconds
   - Include summary statistics at the end (total, mean, min, max)

The script should be idempotent (cleans and rebuilds publications/dataset/ each run). Use shutil.copy2 for file copies. Add `if __name__ == "__main__"` block. Use argparse with `--dry-run` flag.

Important: RDKit's MolFromXYZFile often fails because XYZ files lack connectivity. Instead, extract SMILES from the experimental_shifts.json molecule names and use RDKit to generate InChI/InChIKey from SMILES. If experimental_shifts.json doesn't have SMILES, use the molecule name to look up SMILES (or parse the .nw files which contain the geometry). As a pragmatic fallback, read the XYZ file coordinates and use Chem.rdDetermineBonds.DetermineBonds() to perceive connectivity from 3D coordinates before generating identifiers.
  </action>
  <verify>
Run `python scripts/build_dataset_archive.py` and verify:
- `ls publications/dataset/calculations/ | wc -l` shows 50 compound directories
- `find publications/dataset/calculations/ -name "shielding.nw" | wc -l` shows 610
- `find publications/dataset/calculations/ -name "shielding.out" | wc -l` shows 610
- `wc -l publications/dataset/molecules/molecules.csv` shows 51 lines (header + 50 molecules)
- `wc -l publications/dataset/scaling_factors/scaling_factors.csv` shows 25+ lines (header + 24+ factors)
- `publications/dataset/shielding_tensors/shielding_tensors.json` exists and is valid JSON
- `wc -l publications/dataset/timing/computation_times.csv` shows 611+ lines (header + 610 timings)
- No .movecs or .gridpts files in publications/dataset/
  </verify>
  <done>
publications/dataset/ contains 610 organized calculations (50 compounds x 12 B3LYP solvents + 10 WP04), molecule metadata CSV/JSON with chemical identifiers for all 50 molecules, scaling factor CSV/JSON for all factor sets, aggregated shielding tensor JSON, and computation time CSV for all 610 calculations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run archive build and validate output</name>
  <files>publications/dataset/**</files>
  <action>
Execute the archive build script and validate all outputs:

1. Run `python scripts/build_dataset_archive.py`
2. Verify file counts match expectations
3. Spot-check a few calculations to confirm correct content
4. Verify molecules.csv has all 50 rows with non-empty SMILES/InChI/InChIKey
5. Verify scaling_factors.csv has correct number of factor sets
6. Verify timing data was extracted from all .out files
7. Fix any issues discovered during execution

If InChI generation fails for any molecules, document which ones and use fallback approach (DetermineBonds from 3D coordinates). If timing extraction fails for any .out files, set timing to null/empty and document.
  </action>
  <verify>
- `find publications/dataset/calculations/ -name "*.nw" | wc -l` = 610
- `find publications/dataset/calculations/ -name "*.out" | wc -l` = 610
- `find publications/dataset/calculations/ -name "*.movecs" | wc -l` = 0
- `python -c "import json; d=json.load(open('publications/dataset/shielding_tensors/shielding_tensors.json')); print(len(d))"` shows compound count
- `head -5 publications/dataset/molecules/molecules.csv` shows correct headers
- `head -5 publications/dataset/timing/computation_times.csv` shows correct headers
  </verify>
  <done>
Archive build script runs successfully. All 610 calculations copied, all 50 molecules have chemical identifiers, all scaling factors exported, all shielding tensors aggregated, and computation times extracted. No scratch files leaked into archive.
  </done>
</task>

</tasks>

<verification>
1. `find publications/dataset/calculations/ -name "shielding.nw" | wc -l` returns 610
2. `find publications/dataset/calculations/ -name "shielding.out" | wc -l` returns 610
3. `find publications/dataset/ -name "*.movecs" -o -name "*.gridpts.*" | wc -l` returns 0
4. `publications/dataset/molecules/molecules.csv` has 51 lines (header + 50)
5. `publications/dataset/scaling_factors/scaling_factors.csv` has 25+ lines
6. `publications/dataset/timing/computation_times.csv` has 611+ lines
7. `python -c "import json; json.load(open('publications/dataset/shielding_tensors/shielding_tensors.json'))"` succeeds
8. `python scripts/build_dataset_archive.py --dry-run` completes without errors
</verification>

<success_criteria>
- All 610 NWChem calculations organized in publications/dataset/calculations/ by compound/solvent with canonical naming
- molecules.csv/json contain SMILES, InChI, InChIKey for all 50 molecules
- scaling_factors.csv/json contain all factor sets with regression statistics
- shielding_tensors.json contains aggregated isotropic shielding data
- computation_times.csv contains timing for all 610 calculations
- Build script is reproducible (idempotent, documented, has --dry-run)
</success_criteria>

<output>
After completion, create `.planning/phases/66-dataset-archive-preparation/66-01-SUMMARY.md`
</output>
