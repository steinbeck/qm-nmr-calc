---
context: Ad-hoc GCP deployment debugging
status: in_progress
last_updated: 2026-02-05T21:20:00Z
---

<current_state>
Debugging why a pinene NMR job is still showing "running" after ~30+ minutes on a 64-core GCP VM.
The NMR shielding calculation for conf_001 has been running with 64 MPI processes, but seems stuck or taking longer than expected.
</current_state>

<completed_work>

## MPI Fix (v2.6.5)
- Diagnosed root cause: Inside Docker containers, `nproc` returns 1 even when container has access to all CPUs
- OpenMPI was refusing to run 64 processes due to "not enough slots" error
- Fixed by adding `--oversubscribe` flag to mpirun command in `src/qm_nmr_calc/nwchem/runner.py`
- Committed, tagged v2.6.5, pushed to trigger CI/CD
- Build completed successfully on GitHub Actions

## GCP Deployment
- Destroyed old 32-core VM and data disk
- Created new 64-core VM (n2-standard-64, 256 GB RAM, Spot instance)
- External IP: 34.44.198.243
- Fixed Caddyfile to serve HTTP (no HTTPS since no domain ownership)
- Verified NWCHEM_NPROC=64 and preset shows 64 processes
- Verified MPI can run 64 processes with `--oversubscribe`

## Test Job (2b77a28d7f2c) - Pinene, 64 cores
- Job ID: 2b77a28d7f2c (pinene, CDCl3, production preset, ensemble mode)
- NWChem correctly using 64 cores and ~128 GB RAM (2GB per process)
- **Conformer 1:**
  - Geometry optimization: ~4 min wall time (150s CPU)
  - NMR shielding: ~34 min wall time (1241s CPU, 2056s wall)
- **Conformer 2:**
  - Geometry optimization: ~7 min wall time
  - NMR shielding: in progress when paused (~34 min estimated)
- **Total estimated time for 2 conformers:** ~80-90 min on 64 cores
- Job was still running when session paused
</completed_work>

<remaining_work>

## Immediate Investigation
- Job 2b77a28d7f2c still shows "running" but may be stuck
- Need to check if NMR shielding completed or hung
- Check worker logs for any errors
- Check if conf_002 ever started

## Known Bug (Non-blocking)
- Status bar progress tracking not updating during conformer processing
- Conformer statuses stay "pending" even when complete
- UI shows "Optimizing conformers (0/2)" even after conf_001 finished
- This is a display bug, not affecting calculations

## If Job Failed
- Check NWChem output files for errors
- Check worker container logs
- May need to investigate memory or MPI issues with 64 processes
</remaining_work>

<decisions_made>

- Added `--oversubscribe` to mpirun: Safe because we calculate process count from Python's os.cpu_count() which correctly detects CPUs
- Used n2-standard-64 (64 cores) instead of 60 cores (n2-standard-60 doesn't exist)
- Serving HTTP only via IP (no HTTPS) since user doesn't own steinbeck.net domain
- Caddyfile changed to `:80 { ... }` for plain HTTP access
</decisions_made>

<blockers>

- Job 2b77a28d7f2c taking longer than expected - need to investigate if stuck or just slow
</blockers>

<context>
The v2.6.5 fix (--oversubscribe) was necessary because Docker container cgroup detection causes nproc to return 1,
which makes OpenMPI think there's only 1 slot available. The fix allows MPI to trust our calculated process count.

NWChem was confirmed running with 64 processes at ~99% CPU each. The geometry optimization for conf_001 completed
in about 4 minutes (150s CPU time). The NMR shielding calculation then started but has been running for 25+ minutes.

The user noted that the job "should long be finished" - this needs investigation.
</context>

<next_action>
1. Check current job status: `cat /mnt/disks/data/jobs/2b77a28d7f2c/status.json`
2. Check if NMR shielding completed: `tail -50 .../conf_001/shielding.out`
3. Check for active NWChem processes: `ps aux | grep nwchem`
4. Check worker logs for errors: `docker compose logs worker --tail=50`
5. If job failed, diagnose and potentially resubmit
</next_action>

<files_modified>
- src/qm_nmr_calc/nwchem/runner.py (committed in v2.6.5)
- On VM only (not in repo): /opt/qm-nmr-calc/Caddyfile (changed to HTTP-only)
- On VM only: /opt/qm-nmr-calc/.env (NWCHEM_NPROC=64, no DOMAIN)
</files_modified>

<gcp_resources>
**ALL DELETED on 2026-02-05 to save costs:**
- VM: qm-nmr-calc-vm - DELETED
- Disk: qm-nmr-calc-data - DELETED
- Firewall rules: DELETED

To redeploy, use the gcp/ scripts or follow deployment.md instructions.
</gcp_resources>
